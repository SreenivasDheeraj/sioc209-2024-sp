{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning for Geo/Environmental sciences\n",
    "\n",
    "<center><img src=\"../logo_2.png\" alt=\"logo\" width=\"500\"/></center>\n",
    "\n",
    "<em>*Created with ChapGPT</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture 7: Classification using Convolutional Neural Network\n",
    "\n",
    " - [Recap](#Recap)\n",
    " - [Example 1 - MNIST](#Example-1-MNIST-Digit-Classification)\n",
    " - [Example 2 - CIFAR-10](#Example-2-CIFAR-10-Image-Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This tutorial is modified from\n",
    "- fchollet's example at [Simple MNIST convnet](https://keras.io/examples/vision/mnist_convnet/)\n",
    "- Ekta Sharma's example at [Simple Cifar10 CNN Keras code with 88% Accuracy](https://www.kaggle.com/code/ektasharma/simple-cifar10-cnn-keras-code-with-88-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Convolutional neural networks (CNNs) are a type of neural network that is designed to work with spatial data, such as images. They use convolutional layers to extract features from the data and pooling layers to reduce the dimensionality of the data. CNNs are widely used in computer vision tasks, such as image classification and object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "In this lecture, we will show how to build CNN models to classify images. We will go through this process by looking at two examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Example 1 MNIST Digit Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prepare the data\n",
    "\n",
    "MNIST, short for Modified National Institute of Standards and Technology database, is a widely used benchmark dataset in the field of machine learning and computer vision. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It consists of a collection of **28x28 pixel grayscale** images of handwritten digits (0 through 9), along with corresponding labels indicating the digit each image represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " Originally constructed from a blend of NIST's datasets, MNIST has become a standard dataset for testing and benchmarking machine learning algorithms, particularly for tasks like digit recognition and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras import datasets, layers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load the data and split it between train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "train_images = np.expand_dims(train_images, -1)\n",
    "test_images = np.expand_dims(test_images, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convert class vectors to binary class matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)\n",
    "class_names = [str(digit) for digit in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualizing some of the images from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6,6])\n",
    "for i in range (9):    # for first 25 images\n",
    "  plt.subplot(3, 3, i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid(False)\n",
    "  plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "  plt.xlabel(class_names[np.argmax(train_labels[i])],fontsize=22)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Likelihood and softmax\n",
    "\n",
    "Keep in mind, we are essentially using CNN to approximate a function $y=f(x)$. For classification purpose, the input $x$ is an image and the output is a label (for example, $y\\in\\{0,1,2\\cdots9\\}$ is a digit for the MNIST dataset in example 1 and $y\\in\\{dog,\\cdots,plane\\}$ is a string for the CIFAR 10 dataset in example 2). \n",
    "\n",
    "However, we have be so used to deal with continuous real-valued functions (as in the regression lecture)! What can we do with an output of digit/string?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Key idea**: Instead of using the label as the output, we can use, equivalently, the conditional probability, i.e., the _likelihood_, of the label $Pr(y|x)$, which of course has a real value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We introduce the **softmax** activation function, defined as:\n",
    "\n",
    "$$\\text{softmax}(\\mathbf{z})_i=\\frac{e^{z_i}}{\\sum_{j=1}^N e^{z_j}}$$\n",
    "\n",
    "- $softmax(\\mathbf{z})_i$ is the $i$th element of the output vector.\n",
    "- $N$ is the number of classes.\n",
    "- $z_i$ is the $i$th element of the input vector.\n",
    "\n",
    "Then the likelihood that input $x$ has lebel $y=i$ is:\n",
    "$$Pr(y=i|x)=\\text{softmax}(f(x))_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**: Suppose we have three classes (A, B, C) and the raw scores from a model are [3.0, 1.0, 0.2]. After applying softmax, we might get probabilities like [0.836, 0.113, 0.051]. This suggests a high confidence in class A, moderate confidence in class B, and low confidence in class C. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can build a model by using softmax in the last layer to convert the score into the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))    # num_classes = 10\n",
    "\n",
    "# Checking the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical loss function\n",
    "\n",
    "The loss function for categorical tasks is different from that for regression tasks. For classification tasks, we often use the **categorical cross-entropy** loss function, which is defined as:\n",
    "\n",
    "Categorical Cross-Entropy = $-\\sum_{i=1}^{N} y_i \\log(p_i)$\n",
    "\n",
    "where:\n",
    "- $N$ is the number of classes\n",
    "- $y_i$ is the true label (one-hot encoded) for class $i$\n",
    "- $p_i$ is the predicted probability for class $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The categorical cross-entropy loss function measures the difference between the true label and the predicted probability distribution. It penalizes the model more when it makes a high-confidence wrong prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Categorical metrics\n",
    "\n",
    "The accuracy metric is used to evaluate the performance of a classification model. It is defined as the proportion of correct predictions to the total number of predictions made.\n",
    "\n",
    "Accuracy = $\\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Other metrics like precision, recall, and F1-score can also be used to evaluate the performance of a classification model. For a binary classification problem, the model can either correctly predict a positive or negative class. \n",
    " - The precision metric measures the proportion of true positive predictions to the total number of positive predictions made. \n",
    " - The recall metric measures the proportion of true positive predictions to the total number of actual positive instances.\n",
    " - The F1-score is the harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These cane be extended to multi-class classification problems by averaging the metrics across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_images, train_labels, batch_size=128, epochs=10,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Visualizing the Evaluation\n",
    "\n",
    "We can plot:\n",
    "- Loss Curve to compare the training Loss with the testing Loss over increasing epochs.\n",
    "- Accuracy Curve to Compare the training accuracy with the testing accuracy over increasing epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss curve\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(history.history['loss'], 'black', linewidth=2.0)\n",
    "plt.plot(history.history['val_loss'], 'green', linewidth=2.0)\n",
    "plt.legend(['Training Loss', 'Validation Loss'], fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=10)\n",
    "# plt.yscale('log')\n",
    "plt.title('Loss Curves', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy curve\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(history.history['accuracy'], 'black', linewidth=2.0)\n",
    "plt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=10)\n",
    "plt.ylabel('Accuracy', fontsize=10)\n",
    "plt.title('Accuracy Curves', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluate the trained model\n",
    "\n",
    "We use the trained model to predict the images in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Making the Predictions\n",
    "pred = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once we get 'pred', the likelihood of the input image being 0, 1, ..., or 9, we will simply choose the digit corresponding to the largest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Converting the predictions into label index \n",
    "pred_classes = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can visualise the predictions and compare them with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(7,7))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, 25):\n",
    "    axes[i].imshow(test_images[i], cmap=plt.cm.binary)\n",
    "    axes[i].set_title(\"True: %s \\nPredict: %s\" % (class_names[np.argmax(test_labels[i])], class_names[pred_classes[i]]), fontsize = 12)\n",
    "    axes[i].axis('off')\n",
    "    plt.subplots_adjust(wspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Confusion matrix\n",
    "\n",
    "A more quantitative way to evaluate the performance of a classification model is to use a confusion matrix. A confusion matrix is a table that shows the number of true positive, true negative, false positive, and false negative predictions made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(np.argmax(test_labels, axis=1), pred_classes, normalize='true')\n",
    "plt.pcolormesh(conf_matrix, cmap='Blues')\n",
    "plt.setp(plt.gca(), xticklabels=class_names, yticklabels=class_names, xticks=np.arange(0.5, 10.5), yticks=np.arange(0.5, 10.5),\n",
    "         xlabel='Predicted', ylabel='Actual', title='Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Receiver Operating Characteristic (ROC) curve\n",
    "\n",
    "The ROC curve is a graphical representation of the true positive rate (sensitivity) versus the false positive rate (1-specificity) for a binary classification model at various threshold settings. The area under the ROC curve (AUC) is a measure of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The AUC is a measure of how well the model can distinguish between classes. A model with an AUC of 1 is perfect, while a model with an AUC of 0.5 is no better than random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_labels[:, i], pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('Receiver Operating Characteristic', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluate the trained model\n",
    "\n",
    "Hurrah! The CNN prediction is nearly perfect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class balance\n",
    "\n",
    "In a classification problem, the distribution of classes in the dataset can have a significant impact on the model's performance. If the classes are imbalanced, the model may be biased towards the majority class and perform poorly on the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, let's take a look at the portions of digits in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the total number of samples in the imbalanced dataset\n",
    "total_samples = len(train_labels)\n",
    "\n",
    "# Calculate the number of samples in each class in the imbalanced dataset\n",
    "class_counts = np.sum(train_labels, axis=0)\n",
    "\n",
    "# Calculate the proportions of each class\n",
    "class_proportions = class_counts / total_samples * 100\n",
    "\n",
    "# Plot the proportions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(class_names, class_counts)\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Distribution of Classes in the Original Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see, while the percentage is not exactly the same for different classes, the difference is reasonably small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Imbalanced Data\n",
    "\n",
    "Now, let's create a dataset where the number of samples in each class is not even close, making it imbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(train_images, train_labels), (_, _) = mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "train_images = np.expand_dims(train_images, -1)\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "train_labels_one_hot = to_categorical(train_labels,num_classes)\n",
    "\n",
    "# Define the proportion for each class (make it imbalanced)\n",
    "proportions = {0: 0.14, 1: 0.2, 2: 0.057, 3: 0.36, 4: 0.05, 5: 0.001, 6: 0.01, 7: 0.002, 8: 0.015, 9: 0.165}\n",
    "print(sum(proportions.values()))\n",
    "# Calculate the number of samples needed for each class\n",
    "num_samples_per_class = {}\n",
    "for label, proportion in proportions.items():\n",
    "    num_samples_per_class[label] = int(len(train_labels) * proportion)\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "imbalanced_train_images = []\n",
    "imbalanced_train_labels = []\n",
    "for label, num_samples in num_samples_per_class.items():\n",
    "    indices = np.where(train_labels == label)[0]\n",
    "    selected_indices = np.random.choice(indices, size=num_samples, replace=True)\n",
    "    imbalanced_train_images.extend(train_images[selected_indices])\n",
    "    imbalanced_train_labels.extend(train_labels_one_hot[selected_indices])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "imbalanced_train_images = np.array(imbalanced_train_images)\n",
    "imbalanced_train_labels = np.array(imbalanced_train_labels)\n",
    "\n",
    "# Visualize the proportion of each class\n",
    "class_counts = np.bincount(imbalanced_train_labels.argmax(axis=1))\n",
    "class_names = [str(i) for i in range(10)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(class_names, class_counts)\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Distribution of Classes in the Imbalanced Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What if we use this imbalanced data for classification? We'll still use the same architecture for the model and test data but train the model with the imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "model2.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model2.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dropout(0.5))\n",
    "model2.add(layers.Dense(num_classes, activation='softmax'))    # num_classes = 10\n",
    "\n",
    "# Checking the model summary\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history2 = model2.fit(imbalanced_train_images, imbalanced_train_labels, batch_size=128, epochs=10,  shuffle=True,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss curve\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(history.history['val_loss'], 'green', linewidth=2.0)\n",
    "plt.plot(history2.history['val_loss'], 'green', linewidth=2.0,linestyle='--')\n",
    "plt.legend(['Training Loss', 'Validation Loss','Training Loss (imbalanced data)', 'Validation Loss (imbalanced data)'], fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=10)\n",
    "# plt.yscale('log')\n",
    "plt.title('Loss Curves', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy curve\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)\n",
    "plt.plot(history2.history['val_accuracy'], 'blue', linewidth=2.0,linestyle = '--')\n",
    "plt.legend(['Validation Accuracy', 'Validation Accuracy (imbalanced data)'], fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=10)\n",
    "plt.ylabel('Accuracy', fontsize=10)\n",
    "plt.title('Accuracy Curves', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Making the Predictions\n",
    "pred = model2.predict(test_images)\n",
    "# Converting the predictions into label index \n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(8,8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, 25):\n",
    "    axes[i].imshow(test_images[i], cmap=plt.cm.binary)\n",
    "    axes[i].set_title(\"True: %s \\nPredict: %s\" % (class_names[np.argmax(test_labels[i])], class_names[pred_classes[i]]), fontsize = 12)\n",
    "    axes[i].axis('off')\n",
    "    plt.subplots_adjust(wspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can also visualize the confusion matrix to see how well the model is performing:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(np.argmax(test_labels, axis=1), pred_classes, normalize='true')\n",
    "plt.pcolormesh(conf_matrix, cmap='Blues')\n",
    "plt.setp(plt.gca(), xticklabels=class_names, yticklabels=class_names, xticks=np.arange(0.5, 10.5), yticks=np.arange(0.5, 10.5),\n",
    "         xlabel='Predicted', ylabel='Actual', title='Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dealing with Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can improve the model performance by using techniques like oversampling, undersampling, or using class weights to handle imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Oversampling involves increasing the number of samples in the minority class by generating synthetic samples. Undersampling involves reducing the number of samples in the majority class by randomly selecting a subset of samples. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Class weights are used to assign higher weights to the minority class samples during training. This helps the model to focus more on the minority class and improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Training label weights\n",
    "total_samples = len(imbalanced_train_labels)\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = {}\n",
    "for label, num_samples in num_samples_per_class.items():\n",
    "    class_weights[label] = total_samples / (num_classes * num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history2 = model2.fit(imbalanced_train_images, imbalanced_train_labels, batch_size=128, epochs=10,  shuffle=True,\n",
    "                    validation_data=(test_images, test_labels), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred = model2.predict(test_images)\n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(np.argmax(test_labels, axis=1), pred_classes, normalize='true')\n",
    "plt.pcolormesh(conf_matrix, cmap='Blues')\n",
    "plt.setp(plt.gca(), xticklabels=class_names, yticklabels=class_names, xticks=np.arange(0.5, 10.5), yticks=np.arange(0.5, 10.5),\n",
    "         xlabel='Predicted', ylabel='Actual', title='Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This looks good! The model is able to predict the digits with high accuracy even with imbalanced data. This simple example demonstrates the importance of handling imbalanced data in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2 CIFAR 10 Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "We now consider a more complex dataset: CIFAR-10 which stands for the Canadian Institute for Advanced Research 10, with 60,000 images in total.\n",
    "\n",
    "Unlike MNIST, CIFAR-10 consists of color images, each with a resolution of 32x32 pixels, classified into 10 different classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Checking the number of rows (records) and columns (features)\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "# Checking the number of unique classes \n",
    "print(np.unique(train_labels))\n",
    "print(np.unique(test_labels))\n",
    "\n",
    "# Creating a list of all the class labels\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Again, we can take a look at some of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6,6])\n",
    "for i in range (25):    # for first 25 images\n",
    "  plt.subplot(5, 5, i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid(False)\n",
    "  plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "  plt.xlabel(class_names[train_labels[i][0]], fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Data Preprocessing\n",
    "- The reason for Standardizing/Normalizing is to convert all pixel values to values between 0 and 1.\n",
    "- The reason for converting type to float is that to_categorical (one hot encoding) needs the data to be of type float by default.\n",
    "- The reason for using to_categorical is that the loss function that we will be using in this code (categorical_crossentropy) when compiling the model needs data to be one hot encoded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Converting the pixels data to float type\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    " \n",
    "# Standardizing (255 is the total number of pixels an image can have)\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255 \n",
    "\n",
    "# One hot encoding the target class (labels)\n",
    "num_classes = 10\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's build a CNN model for the CIFAR-10 dataset. It's bigger than the MNIST dataset, so we need a more complex model.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))    # num_classes = 10\n",
    "\n",
    "# Checking the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting the Model\n",
    "\n",
    "- Batch Size is used for Adam optimizer.\n",
    "- Epochs - One epoch is one complete cycle (forward pass + backward pass).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note the smaller batch size because of the larger dataset and larger model\n",
    "history = model.fit(train_images, train_labels, batch_size=64, epochs=10,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Apparently, the training takes much longer time than the MNIST dataset! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualizing the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss curve\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(history.history['loss'], 'black', linewidth=2.0)\n",
    "plt.plot(history.history['val_loss'], 'green', linewidth=2.0)\n",
    "plt.legend(['Training Loss', 'Validation Loss'], fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=10)\n",
    "plt.title('Loss Curves', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy curve\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(history.history['accuracy'], 'black', linewidth=2.0)\n",
    "plt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=10)\n",
    "plt.ylabel('Accuracy', fontsize=10)\n",
    "plt.title('Accuracy Curves', fontsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Oops. Looks like within 10 epochs the training process is not that satisfying. Try to change the epoch number in the CIFAR example to a larger value (say 100) and rerun the training process after class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Still, let's take 25 images from the testing data and see how many of it we predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Making the Predictions\n",
    "pred = model.predict(test_images)\n",
    "print(pred)\n",
    "\n",
    "# Converting the predictions into label index \n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "print(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the Actual vs. Predicted results\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(8,8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, 25):\n",
    "    axes[i].imshow(test_images[i])\n",
    "    axes[i].set_title(\"True: %s \\nPredict: %s\" % (class_names[np.argmax(test_labels[i])], class_names[pred_classes[i]]))\n",
    "    axes[i].axis('off')\n",
    "    plt.subplots_adjust(wspace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can also visualize the confusion matrix to see how well the model is performing:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(np.argmax(test_labels, axis=1), pred_classes, normalize='true')\n",
    "plt.pcolormesh(conf_matrix, cmap='Blues')\n",
    "plt.setp(plt.gca(), xticklabels=class_names, yticklabels=class_names, xticks=np.arange(0.5, 10.5), yticks=np.arange(0.5, 10.5),\n",
    "         xlabel='Predicted', ylabel='Actual', title='Confusion Matrix')\n",
    "plt.gca().xaxis.set_tick_params(rotation=45)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have learned from two examples how to use CNN for image classification. Evaluation of classification models can be done using metrics like accuracy, precision, recall, F1-score, confusion matrix, and ROC curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Depending on the task we might focus more on one metric than the other. For example, in a medical diagnosis task, we might want to minimize false negatives, so we would focus more on recall. For a spam detection task, we might want to minimize false positives, so we would focus more on precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Handling imbalanced data is important for improving the performance of classification models. Techniques like oversampling, undersampling, and class weights can be used to handle imbalanced data."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
